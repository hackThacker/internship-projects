# scanner/crawler.py - FIXED AND COMPLETED VERSION

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

class Crawler:
    def __init__(self, base_url):
        self.base_url = base_url
        self.domain_name = urlparse(base_url).netloc
        self.visited_urls = set()
        self.urls_to_visit = [base_url] # Use a list as a queue to visit URLs

    def start(self):
        """Starts the crawling process from the base URL."""
        while self.urls_to_visit:
            url = self.urls_to_visit.pop(0) # Get the next URL to visit
            if url in self.visited_urls:
                continue
            
            self.visited_urls.add(url)
            print(f"[Crawler] Visiting: {url}")
            
            try:
                response = requests.get(url, timeout=5)
                # Find new links on the current page
                discovered_links = self._find_links(response.content, url)
                for link in discovered_links:
                    if link not in self.visited_urls and link not in self.urls_to_visit:
                        self.urls_to_visit.append(link)
            except requests.RequestException as e:
                print(f"[Crawler] Failed to get URL {url}: {e}")
        
        return list(self.visited_urls)

    def _find_links(self, html_content, current_url):
        """Finds all valid, in-domain links on a page."""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = set()
        for anchor_tag in soup.find_all('a', href=True):
            href = anchor_tag['href']
            # Make the URL absolute
            full_url = urljoin(current_url, href)
            # Remove URL fragments (#) and ensure it's in the same domain
            full_url = full_url.split('#')[0]
            if self.domain_name in full_url:
                links.add(full_url)
        return links

    def get_forms_from_url(self, url):
        """Extracts all forms from a single URL."""
        try:
            response = requests.get(url, timeout=5)
            soup = BeautifulSoup(response.content, 'html.parser')
            forms = soup.find_all('form')
            
            form_details_list = []
            for form in forms:
                details = {}
                action = form.get('action')
                method = form.get('method', 'get').lower()
                
                details['action_url'] = urljoin(url, action) if action else url
                details['method'] = method
                details['inputs'] = []

                for input_tag in form.find_all(['input', 'textarea', 'select']):
                    input_type = input_tag.get('type', 'text')
                    input_name = input_tag.get('name')
                    details['inputs'].append({'type': input_type, 'name': input_name})
                form_details_list.append(details)
            return form_details_list
        except requests.RequestException:
            return []