# app.py - FINAL HIGH-CONCURRENCY VERSION

from flask import Flask, render_template, request, Response, stream_with_context, redirect, url_for
from scanner.crawler import Crawler
# IMPORTANT: Import the new asynchronous scanner classes
from scanner.xss_scanner_async import AsyncXSSScanner
from scanner.sqli_scanner_async import AsyncSQLiScanner
from scanner.report_generator import generate_report_html
import time
import threading
import queue # For thread-safe communication

app = Flask(__name__)
scan_results_storage = {}

# (Keep /index, /start-scan, and /scanning routes exactly the same as the previous version)

@app.route('/', methods=['GET'])
def index():
    return render_template('index.html')

@app.route('/start-scan', methods=['POST'])
def start_scan():
    base_url = request.form.get('url')
    if not base_url: return "Error: URL is required.", 400
    return redirect(url_for('scanning_page', url=base_url))

@app.route('/scanning')
def scanning_page():
    target_url = request.args.get('url')
    return render_template('scanning.html', target_url=target_url)

def _perform_scan(base_url):
    """
    Generator function that manages the scan threads and yields log messages
    from a thread-safe queue.
    """
    log_queue = queue.Queue() # Create a queue to get messages from scanner threads
    all_vulnerabilities = []

    def log_message(message, css_class=""):
        return f'<span class="{css_class}">{message}</span>'

    try:
        # --- Phase 1: Crawl (Synchronous) ---
        log_queue.put(log_message('[*] Phase 1: Crawling website...', 'log-crawl'))
        crawler = Crawler(base_url)
        all_discovered_links = crawler.start()
        log_queue.put(log_message(f'[*] Crawl Complete. Discovered {len(all_discovered_links)} unique links.', 'log-crawl'))

        # --- Phase 2: High-Concurrency Asynchronous Scanning ---
        log_queue.put(log_message('\n[*] Phase 2: Launching High-Speed Scanners...', 'log-phase'))

        # Create scanner instances
        xss_scanner = AsyncXSSScanner(all_discovered_links, log_queue)
        sqli_scanner = AsyncSQLiScanner(all_discovered_links, log_queue)

        # Create threads to run the asyncio event loops
        xss_thread = threading.Thread(target=xss_scanner.run)
        sqli_thread = threading.Thread(target=sqli_scanner.run)

        # Start the scanner threads
        xss_thread.start()
        sqli_thread.start()
        
        # Monitor the queue for log messages while threads are running
        while xss_thread.is_alive() or sqli_thread.is_alive():
            try:
                # Get a message from the queue, with a 1-second timeout
                message = log_queue.get(timeout=1)
                yield f"data: {message}\n\n"
            except queue.Empty:
                # If queue is empty, loop continues to check thread status
                continue
        
        # Wait for threads to completely finish
        xss_thread.join()
        sqli_thread.join()
        
        # Drain any remaining messages from the queue
        while not log_queue.empty():
            message = log_queue.get()
            yield f"data: {message}\n\n"
            
        # Collect results from the scanners
        all_vulnerabilities.extend(xss_scanner.vulnerabilities)
        all_vulnerabilities.extend(sqli_scanner.vulnerabilities)

        # --- Phase 3: Finalize ---
        scan_duration = time.time() - start_time
        scan_results_storage['results'] = {
            "target_url": base_url, "vulnerabilities": all_vulnerabilities,
            "scan_duration": round(scan_duration, 2), "total_vulns": len(all_vulnerabilities)
        }
        yield f"event: scan-complete\ndata: {log_message(f'Found {len(all_vulnerabilities)} vulnerabilities. Preparing report...', 'log-complete')}\n\n"

    except Exception as e:
        error_message = f"A critical error occurred: {e}"
        yield f"data: {log_message(error_message, 'log-error')}\n\n"
        yield f"event: scan-complete\ndata: {log_message('Scan failed.', 'log-error')}\n\n"

@app.route('/stream-scan')
def stream_scan():
    base_url = request.args.get('url')
    # stream_with_context is crucial for making generators work in Flask
    return Response(stream_with_context(_perform_scan(base_url)), mimetype='text/event-stream')

@app.route('/report')
def report():
    results = scan_results_storage.get('results')
    if not results: return "No scan results found. Please start a new scan.", 404
    return generate_report_html(**results)

if __name__ == '__main__':
    start_time = time.time() # To be used by the _perform_scan generator
    app.run(debug=True, threaded=True)